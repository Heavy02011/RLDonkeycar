Can Neural stick be run in compiled mode with revised parameters?
  - not on RPi



------------------
Step 1: initial training
RPI1
  - Runs CV2 line following
  - get reward
  - send ROI(n) to rpi2 with steering/throttle
  - send ROI(n+1) to rpi2
  - controls car
  - stops if outside track

RPI2
  - receives ROI(n), thottle, steering 
  - receives ROI(N+1) reward
  - trains as fast as possible
  - drops on floor what it can't handle
  - no car control

------------------
Step 2: RPI to RPI  Model transfer

RPI2 sends RPI1 the model after X (thousands
   30sec/sec x 20frames x 10 laps = 6000 images trained
RPI1 continues to control car until model is ready

------------------

Step 3: NN takes control

RPI1:
  - increase FPS hertz
  - NN takes control from CV2
  - typically uses current model to control car
  - when get READY from RPI2:
    -- Do NN to get steering/throttle
    -- randomly change steering/throttle by small delta (GPS)
    -- sends ROI(n),steering/throttle to RPI2
    -- sends ROI(n+1) to rpi2 

  - random delta throttle: +2 / +1/  0/ -1
                           10%/50%/30%/10%
    random delta steering: -2 / -1/  0/ +1/ +2
      with distribution:   10% 20% 40% 20% 10%
    note: 12% of 0 delta throttle/0 delta steering.
            Just run with reward anyway
    if steering already at +-Max:
            increase odds of steering to start at Max

RPI2:
  - send Ready to RPI1
  - receives ROI(n) from rpi1 with GPS steering/throttle
  - receives ROI(n+1) from rpi1 
  - computes ROI(n+1) reward
     -- if out of bounds send "Stop" to RPI1
  - feedback/train model

------------------
Step 4: RPI to RPI  Model transfer

RPI2 sends revised model to RPI1 after:
  - X (1000) random images
  - or if stopped 
RPI1 continues to control car until model is ready
RPI1 switches

------------------

Deep Copy Keras/Tensorflow 
https://github.com/keras-team/keras/issues/1765

model_copy = keras.models.clone_model(model)
model_copy.set_weights(model.get_weights())

https://stackoverflow.com/questions/48547688/tensorflow-keras-copy-weights-from-one-model-to-another
Confirms above. clone_model may not be necessary; just transfer weights.
Keras FAQ covers saving and loading model weights. You can save/load all weights or you can go by layer as well: 
https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model

You can use model.save(filepath) to save a Keras model into a single HDF5 file which will contain:

    the architecture of the model, allowing to re-create the model
    the weights of the model
    the training configuration (loss, optimizer)
    the state of the optimizer, allowing to resume training exactly where you left off.

You can then use keras.models.load_model(filepath) to reinstantiate your model. load_model will also take care of compiling the model using the saved training configuration (unless the model was never compiled in the first place).


-------------------------------
Battery Voltage Depletion
 - Change Throttle to +-1 increments instead of %? 
   -- probably not
   -- allows reward to be assigned on +- 10 throttle catagorical.
 - probably use a persistent "lap counter". probably not.
 - or figure out from initial start-moving throttle. Default. 
 - allow a battery_compensation command-line input

 - throttle should be based on "min throttle"

Use minimum throttle as state variable, battery comp as two more variables?

"Model-Based": change only 1 of steering or throttle. 
  
  - NN doesn't know center!  When other pi is ready:
    - bound to NN output, with following deltas:
      If steering nearly straight, allow +2 option
      random delta throttle: +2 / +1/  0/ -1
                             10%/50%/30%/10%      
      If steering angle > 20:
      random delta throttle:      +1/  0/ -1
                                 60%/30%/10%      
      random delta steering: -2 / -1/  0/ +1/ +2 
        with distribution:   10% 20% 40% 20% 10%
      note: 12% of 0 delta throttle/0 delta steering. 
            Just run with reward anyway
      if steering already at +-Max:
            increase odds of steering to start at Max

  - when stop/converge? 


  Training Pi
  - use absolute throttle min + 15 states
  - use absolute steering 15 states
  - Do combined 225 states? Or two 15 state variables?  Probably latter.

  - center -> lane width pixel / 3 -> give some delta (dotted line difference)
    outside third -> give a penalty
    reward_center = 3*lw/4 - abs(center - x) 
    if no center, rw = -infity
    reward_throttle = throttle - minthrottle - battery_comp = 1
    reward = reward_center * reward_throttle
  - track

  Modes:
  - start with last model, recompute throttle min
  - transfer model when OOB / stopped


-------------------------------
https://stackoverflow.com/questions/45924454/reinforcement-learning-a3c-with-multiple-independent-outputs
https://stackoverflow.com/questions/43881897/in-reinforcement-learning-what-is-the-policy-gradient-when-multiple-actions-are


https://arxiv.org/pdf/1509.02971.pdf 
https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html
https://github.com/EdwardTyantov/ultrasound-nerve-segmentation/blob/master/metric.py
-------------------------------
  def GPS_learn_

    def gps_trial(self):
        if gps_server_state == READY:
          return True
        else:
          return False

    def run(self, img_arr):
        img_arr = img_arr.reshape((1,) + img_arr.shape)
        angle_binned, throttle_binned = self.model.predict(img_arr)
        #print('throttle', throttle)
        #angle_certainty = max(angle_binned[0])
        angle_unbinned = dk.utils.linear_unbin(angle_binned)
        throttle_unbinned = dk.utils.linear_unbin(throttle_binned)
        MAXUNBINNED = 15
        if (gps_trial):
          rdm = np.random.random() * 10
          # random delta throttle: +2 / +1/  0/ -1
          #                        10%/50%/30%/10%
          # If steering angle > 20:
          # random delta throttle: +1 / +1/  0/ -1
          if rdm < 1:
            if (angle_unbinned <= 20)
              delta_throttle = 2
            else:
              delta_throttle = 1
          elif rdm < 6:
            delta_throttle = 1
          elif rdm < 9:
            delta_throttle = 0
          else:
            delta_throttle = -1
          rdm = np.random.random() * 10
          # random delta steering: -2 / -1/  0/ +1/ +2
          #   with distribution:   10% 20% 40% 20% 10%
          if rdm < 1:
            delta_steering = -2
          elif rdm < 3:
            delta_steering = -1 
          elif rdm < 5:
            delta_steering = 0 
          elif rdm < 9:
            delta_steering = 1
          else:
            delta_steering = 2
          angle_unbinned = max(min(angle_unbinned + delta_steering, 0),MAXUNBINNED)
          throttle_unbinned = max(min(throttle_unbinned + delta_throttle, 0),MAXUNBINNED)
        return angle_unbinned, throttle_unbinned

####
    # kl
    def compute_reward(self, img_arr, throttle, steering):
        global width, laneWidth, minthrottle, battery_adjustment
        global pixPerFrame, bestll, bestcl, bestrl

        ll = LaneLines()
        simplecl, lines, roi = ll.process_img(img)
        if simplecl is not None:
          dist_from_center = abs(self.closestX(cl) - (width/2))
        elif lines is not None:
          steering, throttle = self.lrclines(lines,roi)
          if bestcl is not None:
            dist_from_center = abs(self.closestX(bestcl) - (width/2))
          elif bestll is not None and bestrl is not None:
            dist_from_center = abs((self.closestX(bestll) + self.closestX(bestrl))/2 - (width/2))
          elif bestll is not None:
            dist_from_center = abs((self.closestX(bestll) + laneWidth) - (width/2))
          elif bestrl is not None:
            dist_from_center = abs((self.closestX(bestrl) - laneWidth) - (width/2))
        else:
          return STOP            # STOP!!
        MIN_DIST_FROM_CENTER = 20
        if dist_from_center < MIN_DIST_FROM_CENTER:
          dist_from_center = 0
        # steering = min(max((self.closestX(cl) - midpix) / denom,-1),1)
        reward_center = 3*lineWidth/4 - dist_from_center 
        reward_throttle = throttle - minthrottle - battery_adjustment + 1
        reward = reward_center * reward_throttle
        return reward
       
###
  # params sent in msg from control_pi
  # run at RL_pi
  def GPS_learn(self, trial_roi, results_roi, trial_angle, trial_throttle):
    # note: trial_throttle is the raw 0-14 bin , adjusted for minthrottle
    # and battery:
    # trial_bin = throttle - minthrottle - battery_adjustment + 1
    reward = self.compute_reward(results_roi, trial_throttle, trial_steering)
    # flattened number of potential outputs = 15 * 15 = 225
    index = (trial_throttle * 15 - 1) + trial_steering
    output[index] = reward
    model.fit(trial_roi, output.reshape(-1, 225), epochs=1, verbose=0)



    kl = RL()
    kl.run
    model.fit(
np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)

    # now execute the q learning
    y = 0.95
    eps = 0.5
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        if i % 100 == 0:
            print("Episode {} of {}".format(i + 1, num_episodes))
        done = False
        r_sum = 0
        while not done:
            if np.random.random() < eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))
            # reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.
            new_s, r, done, _ = env.step(a)
            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)
            s = new_s
            r_sum += r
        r_avg_list.append(r_sum / 1000)
-------------------------------

  def GPS_learn(model):
    for i in range(num_episodes):
      # env.reset is from the gym.openai.com test environments
      s = env.reset()
      eps *= decay_factor
      if i % 100 == 0:
          print("Episode {} of {}".format(i + 1, num_episodes))
      done = False
      r_sum = 0
      while not done:
          if np.random.random() < eps:
              a = np.random.randint(0, 2)
          else:
              # use the Keras model to produce the two Q values – 
              # one for each possible state. It does this by calling the 
              # model.predict() function. Here the numpy identity 
              # function is used, with vector slicing, to produce the 
              # one-hot encoding of the current state s. The standard 
              # numpy argmax function is used to select the action with 
              # the highest Q value returned from the Keras model 
              # prediction.
              a = np.argmax(model.predict(np.identity(5)[s:s + 1]))

          # env.reset is from the gym.openai.com test environments
          new_s, r, done, _ = env.step(a)
          # Q learning updating rule is the reward r plus the discounted 
          # maximum of the predicted Q values for the new state, new_s.

# 2 images n, n+1 ? => not a simple binary array
# r =  max((throttle - min_throttle), lane_width) / max(pix_from_center, 1)
#      what if throttle gets huge. Can it exceed pix from center? 
# r should be from what to what? looks like 0, 2, 10 in NChain
#
# Does

          target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
s:s+1 => a number?
np.identity(5)[news:news+1] = 5x5 identity matrix 
>>> import numpy as np
>>> s = 4
>>> print(np.identity(5)[s:s+1])
[[0. 0. 0. 0. 1.]]
# 1-hot encoding of state
>>> s = 8
>>> print(np.identity(5)[s:s+1])
  target_vec = model.predict(np.identity(5)[s:s + 1])[0]
predict's input is a 1-high
          # target_vec extracts both predicted Q values for state s. 
          target_vec[a] = target
          # next update the Keras model a single training step. 
          # np.identity(5)[s:s + 1] is current state
          #   i.e. the one-hot encoded input to the model. 
          # target vector is reshaped to required dimensions of (1, 2). 
          # epochs=1 tells the fit function that we only want to train 
          # for a single iteration 
          model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)

          s = new_s
          r_sum += r
      r_avg_list.append(r_sum / 1000)
  
-------------------------------
# for Donkey

  def Q_learn(model):
    # now execute the q learning. In below example, input is a 1-hot 
    # vector where state1 is [0,1,0,0] and state3 is [0,0,0,1,0].
    # output is the qvalues for the 2 possible actions.
    # from http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
    '''
    DK has outputs of steering, throttle 
    use softmax to get best of 15 categorical output of the angle
   
    # continous output of throttle

    '''

    q_table = np.zeros((5, 2))
    y = 0.95
    eps = 0.5  # 50-50 chance, decaying over time, to allow random exploration of state
    lr = 0.8  # learning rate
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
      # env.reset is from the gym.openai.com test environments
      s = env.reset()
      eps *= decay_factor
      if i % 100 == 0:
          print("Episode {} of {}".format(i + 1, num_episodes))
      done = False
      r_sum = 0
      while not done:
          if np.random.random() < eps:
              # random # < 1, eps = .999 only 1/1000 chance?
              a = np.random.randint(0, 2)
random # either 0 or 1
          else:
              # use the Keras model to produce the two Q values – 
              # one for each possible state. It does this by calling the 
              # model.predict() function. Here the numpy identity 
              # function is used, with vector slicing, to produce the 
              # one-hot encoding of the current state s. The standard 
              # numpy argmax function is used to select the action with 
              # the highest Q value returned from the Keras model 
              # prediction.
              a = np.argmax(model.predict(np.identity(5)[s:s + 1]))


          # env.reset is from the gym.openai.com test environments
          new_s, r, done, _ = env.step(a)
          # Q learning updating rule is the reward r plus the discounted 
          # maximum of the predicted Q values for the new state, new_s.

          # r = 0, 2, or 10

          # discounted reward from future predicted state is:
          #   y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))

          target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
          # alter the value for only the action a which has been chosen
          target_vec = model.predict(np.identity(5)[s:s + 1])[0]
          # target_vec extracts both predicted Q values for state s. 
          target_vec[a] = target
          # next update the Keras model a single training step. 
          # np.identity(5)[s:s + 1] is current state
          #   i.e. the one-hot encoded input to the model. 
          # target vector is reshaped to required dimensions of (1, 2). 
          # epochs=1 tells the fit function that we only want to train 
          # for a single iteration 
          model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)

          s = new_s
          r_sum += r
      # there are 1000 steps in example
      r_avg_list.append(r_sum / 1000)
  
----------------
# ORIG
  def Q_learn(model):
    # now execute the q learning. In below example, input is a 1-hot 
    # vector where state1 is [0,1,0,0] and state3 is [0,0,0,1,0].
    # output is the qvalues for the 2 possible actions.
    # from http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
    decay_factor = 0.999
      # env.reset is from the gym.openai.com test environments
      s = env.reset()
      eps *= decay_factor
      if i % 100 == 0:
          print("Episode {} of {}".format(i + 1, num_episodes))
      done = False
      r_sum = 0
      while not done:
          if np.random.random() < eps:
              a = np.random.randint(0, 2)
          else:
              # use the Keras model to produce the two Q values – 
              # one for each possible state. It does this by calling the 
              # model.predict() function. Here the numpy identity 
              # function is used, with vector slicing, to produce the 
              # one-hot encoding of the current state s. The standard 
              # numpy argmax function is used to select the action with 
              # the highest Q value returned from the Keras model 
              # prediction.
              a = np.argmax(model.predict(np.identity(5)[s:s + 1]))
returns array of prediction

          # env.reset is from the gym.openai.com test environments
          new_s, r, done, _ = env.step(a)
          # Q learning updating rule is the reward r plus the discounted 
          # maximum of the predicted Q values for the new state, new_s.
          target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
Only 5 states in nchain!
Only 2 possible actions: 0,1
          # alter the value for only the action a which has been chosen
          target_vec = model.predict(np.identity(5)[s:s + 1])[0]
          # target_vec extracts both predicted Q values for state s. 
          target_vec[a] = target
          # next update the Keras model a single training step. 
          # np.identity(5)[s:s + 1] is current state
          #   i.e. the one-hot encoded input to the model. 
          # target vector is reshaped to required dimensions of (1, 2). 
          # epochs=1 tells the fit function that we only want to train 
          # for a single iteration 
          model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)

          s = new_s
          r_sum += r
      r_avg_list.append(r_sum / 1000)
  
----------------
https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/
Regression Predictions

# example of training a final regression model
from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_regression
from sklearn.preprocessing import MinMaxScaler
# generate regression dataset
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=1)
scalarX, scalarY = MinMaxScaler(), MinMaxScaler()
scalarX.fit(X)
scalarY.fit(y.reshape(100,1))
X = scalarX.transform(X)
y = scalarY.transform(y.reshape(100,1))
# define and fit the final model
model = Sequential()
model.add(Dense(4, input_dim=2, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='linear'))
model.compile(loss='mse', optimizer='adam')
model.fit(X, y, epochs=1000, verbose=0)
############# 
# new instances where we do not know the answer
Xnew, a = make_regression(n_samples=3, n_features=2, noise=0.1, random_state=1)
Xnew = scalarX.transform(Xnew)
# make a prediction
ynew = model.predict(Xnew)
# show the inputs and predicted outputs
for i in range(len(Xnew)):
	print("X=%s, Predicted=%s" % (Xnew[i], ynew[i]))
############ single prediction
# new instance where we do not know the answer
Xnew = array([[0.29466096, 0.30317302]])
# make a prediction
ynew = model.predict(Xnew)
# show the inputs and predicted outputs
print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))

-------------------------------

ETHERNET

     
The address needs to be unique for each Raspberry Pi. And the gateway is not needed if there is no router with internet access connected.

So it looks something like this.

auto eth0
iface eth0 inet static
    address 192.168.12.7
    netmask 255.255.255.0
    gateway 192.168.12.254

https://www.raspberrypi.org/learning/networking-lessons/lessons/
http://zeromq.org/  => message queues
https://wiki.python.org/moin/TcpCommunication

https://stackoverflow.com/questions/49294156/python-3-6-zeromq-pyzmq-asyncio-pub-sub-hello-world
https://openwsn.atlassian.net/wiki/spaces/OW/pages/113475628/Sending+Receiving+messages+on+the+EventBus+using+ZMQ

https://stackoverflow.com/questions/10252010/serializing-class-instance-to-json



imge in Json: You'll have to wrap your binary data in a text-safe encoding first, such as Base-64:

json.dumps({'picture' : data.encode('base64')})

Note that base64.b64encode() does not automatically split the encoded data, so any decoder that is used must be able to handle one very long line.

https://stackoverflow.com/questions/38583622/python-read-json-object-over-tcp-connection-using-regex
  160x120 = 15200 chars max

https://stackoverflow.com/questions/34279519/sending-various-types-of-data-through-sockets-in-python
What you want(ed) is ijson, an incremental json parser. It is available here: https://pypi.python.org/pypi/ijson/

https://github.com/mpv-player/mpv/blob/master/DOCS/man/ipc.rst
https://pypi.org/project/json-rpc/
