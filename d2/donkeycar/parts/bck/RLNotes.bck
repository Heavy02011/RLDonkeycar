Can Neural stick be run in compiled mode with revised parameters?

------------------
Step 1: initial training
RPI1
  - Runs CV2 line following
  - get reward
  - send ROI(n) to rpi2 with steering/throttle
  - send ROI(n+1) to rpi2
  - controls car
  - stops if outside track

RPI2
  - receives ROI(n), thottle, steering 
  - receives ROI(N+1) reward
  - trains as fast as possible
  - drops on floor what it can't handle
  - no car control

------------------
Step 2: RPI to RPI  Model transfer

RPI2 sends RPI1 the model after X (thousands
   30sec/sec x 20frames x 10 laps = 6000 images trained
RPI1 continues to control car until model is ready

------------------

Step 3: NN takes control

RPI1:
  - NN takes control from CV2
  - typically uses current model to control car
  - when get READY from RPI2:
    -- Do NN to get steering/throttle
    -- randomly change steering/throttle by small delta (GPS)
    -- sends ROI(n),steering/throttle to RPI2
    -- sends ROI(n+1) to rpi2 

RPI2:
  - send Ready to RPI1
  - receives ROI(n) from rpi1 with GPS steering/throttle
  - receives ROI(n+1) from rpi1 
  - computes ROI(n+1) reward
     -- if out of bounds send "Stop" to RPI1
  - feedback/train model

------------------
Step 4: RPI to RPI  Model transfer

RPI2 sends revised model to RPI1 after:
  - X (1000) random images
  - or if stopped 
RPI1 continues to control car until model is ready
RPI1 switches

------------------

Deep Copy Keras/Tensorflow 
https://github.com/keras-team/keras/issues/1765

model_copy = keras.models.clone_model(model)
model_copy.set_weights(model.get_weights())

https://stackoverflow.com/questions/48547688/tensorflow-keras-copy-weights-from-one-model-to-another
Confirms above. clone_model may not be necessary; just transfer weights.
Keras FAQ covers saving and loading model weights. You can save/load all weights or you can go by layer as well: 
https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model

You can use model.save(filepath) to save a Keras model into a single HDF5 file which will contain:

    the architecture of the model, allowing to re-create the model
    the weights of the model
    the training configuration (loss, optimizer)
    the state of the optimizer, allowing to resume training exactly where you left off.

You can then use keras.models.load_model(filepath) to reinstantiate your model. load_model will also take care of compiling the model using the saved training configuration (unless the model was never compiled in the first place).


-------------------------------
Battery Voltage Depletion
 - Change Throttle to +-1 increments instead of %? 
   -- probably not
   -- allows reward to be assigned on +- 10 throttle catagorical.
 - probably use a persistent "lap counter". probably not.
 - or figure out from initial start-moving throttle. Default. 
 - allow a battery_compensation command-line input

 - throttle should be based on "min throttle"

 150 pwm for steer (10 each!)

 use normal curve for random throttle?
	 use non-linear curve to interpolate throttle?
         -4 -2 -1 -.5 0 .5 1 2 4
          (-4,-4),(-2,-3),(-1,-2),(-.5,-1), (0,0),(.5,1),(1,2),(2,3),(4,4)

4      X               X

3

2        X           X

1          X       X
.5           X   X
0              X

9x15 = 135
Bonus if sum (last 5 rewards ) > sum (prev 5 rewards)?
reward if OOB = -80 * speed?

Distribution Steering (N/A)
Random distribution Throttle (1, 2, 4, 8, 16, 8, 4, 2, 1) -> out of 36

Make "0" = minimum + 7 then use Throttle as absolute, with battery compensation
                     ^can make variable

Use minimum throttle as state variable, battery comp as two more variables?
        -> make part of training data?

target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
What is state? The whole image?


"Model-Based": change only 1 of steering or throttle. 
  
  - NN doesn't know center!  When other pi is ready:
    - bound to NN output, with following deltas:
      random delta throttle: +2 / +1/  0/ -1
                             10%/50%/30%/10%      
      random delta steering: -2 / -1/  0/ +1/ +2 
        with distribution:   10% 20% 40% 20% 10%
      note: 12% of 0 delta throttle/0 delta steering. 
            Just run with reward anyway
      if steering already at +-Max:
            increase odds of steering to start at Max

  - when stop/converge? 

  Training Pi
  - use absolute throttle min + 15 states
  - use absolute steering 15 states
  - Do combined 225 states? Or two 15 state variables?  Probably latter.

  - center -> lane width pixel / 3 -> give some delta (dotted line difference)
    outside third -> give a penalty
    reward_center = 3*lw/4 - abs(center - x) 
    if no center, rw = -infity
    reward_throttle = throttle - minthrottle - battery_comp = 1
    reward = reward_center * reward_throttle
  - track

  Modes:
  - start with last model, recompute throttle min
  - transfer model when OOB / stopped

      XXXXXXXXXXXXXXXXXXXXXXXXX
      If throttle + 1 run before but ran worse, then try angle
          Steering + 1 with throttle + 1

      +1/0/-1 on Steering, +1/0/-1 on Throttle 
           -> if +1 previously tested at this throttle, try -1
           -> if -1 previously tested, try +1
           -> no 0 steer/0 throttle?
           -> just try again next frame?
    - Relative to expected value
      
    - doesn't work without :
      - Increment Throttle until off-center
        (store min, throttle, compensation, ppf) - compensation will be non-linear
      - Turn into line if off-center
      - if near OOB, the reward goes negative
      - Treat steering and throttle as independent variables? no
      - bound changes - use distribution
      - store using combined delta-throttle/steering state bin
 XXXXXXXXXXXXXXXXXXXXXXXXX

On

-------------------------------
https://stackoverflow.com/questions/45924454/reinforcement-learning-a3c-with-multiple-independent-outputs
https://stackoverflow.com/questions/43881897/in-reinforcement-learning-what-is-the-policy-gradient-when-multiple-actions-are


https://arxiv.org/pdf/1509.02971.pdf 
https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html
https://github.com/EdwardTyantov/ultrasound-nerve-segmentation/blob/master/metric.py
-------------------------------
# for Donkey GPS
  # only 1 chosen based on reward function
  # use for both throttle and angle
  # use reward table
    A    S   Reward
   15 x 15                   225 reward table for each frame!
   15 x 10                   150 reward table for each frame!



  def GPS_learn(model):
    for i in range(num_episodes):
      # env.reset is from the gym.openai.com test environments
      s = env.reset()
      eps *= decay_factor
      if i % 100 == 0:
          print("Episode {} of {}".format(i + 1, num_episodes))
      done = False
      r_sum = 0
      while not done:
          if np.random.random() < eps:
              a = np.random.randint(0, 2)
          else:
              # use the Keras model to produce the two Q values – 
              # one for each possible state. It does this by calling the 
              # model.predict() function. Here the numpy identity 
              # function is used, with vector slicing, to produce the 
              # one-hot encoding of the current state s. The standard 
              # numpy argmax function is used to select the action with 
              # the highest Q value returned from the Keras model 
              # prediction.
              a = np.argmax(model.predict(np.identity(5)[s:s + 1]))

          # env.reset is from the gym.openai.com test environments
          new_s, r, done, _ = env.step(a)
          # Q learning updating rule is the reward r plus the discounted 
          # maximum of the predicted Q values for the new state, new_s.

# 2 images n, n+1 ? => not a simple binary array
# r =  max((throttle - min_throttle), lane_width) / max(pix_from_center, 1)
#      what if throttle gets huge. Can it exceed pix from center? 
# r should be from what to what? looks like 0, 2, 10 in NChain
#
# Does

          target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
          # alter the value for only the action a which has been chosen
          target_vec = model.predict(np.identity(5)[s:s + 1])[0]
          # target_vec extracts both predicted Q values for state s. 
          target_vec[a] = target
          # next update the Keras model a single training step. 
          # np.identity(5)[s:s + 1] is current state
          #   i.e. the one-hot encoded input to the model. 
          # target vector is reshaped to required dimensions of (1, 2). 
          # epochs=1 tells the fit function that we only want to train 
          # for a single iteration 
          model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)

          s = new_s
          r_sum += r
      r_avg_list.append(r_sum / 1000)
  
-------------------------------
# for Donkey

  def Q_learn(model):
    # now execute the q learning. In below example, input is a 1-hot 
    # vector where state1 is [0,1,0,0] and state3 is [0,0,0,1,0].
    # output is the qvalues for the 2 possible actions.
    # from http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
    '''
    DK has outputs of steering, throttle 
    use softmax to get best of 15 categorical output of the angle
   
    # continous output of throttle

    '''
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
      # env.reset is from the gym.openai.com test environments
      s = env.reset()
      eps *= decay_factor
      if i % 100 == 0:
          print("Episode {} of {}".format(i + 1, num_episodes))
      done = False
      r_sum = 0
      while not done:
          if np.random.random() < eps:
              a = np.random.randint(0, 2)
          else:
              # use the Keras model to produce the two Q values – 
              # one for each possible state. It does this by calling the 
              # model.predict() function. Here the numpy identity 
              # function is used, with vector slicing, to produce the 
              # one-hot encoding of the current state s. The standard 
              # numpy argmax function is used to select the action with 
              # the highest Q value returned from the Keras model 
              # prediction.
              a = np.argmax(model.predict(np.identity(5)[s:s + 1]))

          # env.reset is from the gym.openai.com test environments
          new_s, r, done, _ = env.step(a)
          # Q learning updating rule is the reward r plus the discounted 
          # maximum of the predicted Q values for the new state, new_s.
          target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
          # alter the value for only the action a which has been chosen
          target_vec = model.predict(np.identity(5)[s:s + 1])[0]
          # target_vec extracts both predicted Q values for state s. 
          target_vec[a] = target
          # next update the Keras model a single training step. 
          # np.identity(5)[s:s + 1] is current state
          #   i.e. the one-hot encoded input to the model. 
          # target vector is reshaped to required dimensions of (1, 2). 
          # epochs=1 tells the fit function that we only want to train 
          # for a single iteration 
          model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)

          s = new_s
          r_sum += r
      r_avg_list.append(r_sum / 1000)
  
----------------
# ORIG
  def Q_learn(model):
    # now execute the q learning. In below example, input is a 1-hot 
    # vector where state1 is [0,1,0,0] and state3 is [0,0,0,1,0].
    # output is the qvalues for the 2 possible actions.
    # from http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
      # env.reset is from the gym.openai.com test environments
      s = env.reset()
      eps *= decay_factor
      if i % 100 == 0:
          print("Episode {} of {}".format(i + 1, num_episodes))
      done = False
      r_sum = 0
      while not done:
          if np.random.random() < eps:
              a = np.random.randint(0, 2)
          else:
              # use the Keras model to produce the two Q values – 
              # one for each possible state. It does this by calling the 
              # model.predict() function. Here the numpy identity 
              # function is used, with vector slicing, to produce the 
              # one-hot encoding of the current state s. The standard 
              # numpy argmax function is used to select the action with 
              # the highest Q value returned from the Keras model 
              # prediction.
              a = np.argmax(model.predict(np.identity(5)[s:s + 1]))

          # env.reset is from the gym.openai.com test environments
          new_s, r, done, _ = env.step(a)
          # Q learning updating rule is the reward r plus the discounted 
          # maximum of the predicted Q values for the new state, new_s.
          target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
          # alter the value for only the action a which has been chosen
          target_vec = model.predict(np.identity(5)[s:s + 1])[0]
          # target_vec extracts both predicted Q values for state s. 
          target_vec[a] = target
          # next update the Keras model a single training step. 
          # np.identity(5)[s:s + 1] is current state
          #   i.e. the one-hot encoded input to the model. 
          # target vector is reshaped to required dimensions of (1, 2). 
          # epochs=1 tells the fit function that we only want to train 
          # for a single iteration 
          model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)

          s = new_s
          r_sum += r
      r_avg_list.append(r_sum / 1000)
  
----------------
https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/
Regression Predictions

# example of training a final regression model
from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_regression
from sklearn.preprocessing import MinMaxScaler
# generate regression dataset
X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=1)
scalarX, scalarY = MinMaxScaler(), MinMaxScaler()
scalarX.fit(X)
scalarY.fit(y.reshape(100,1))
X = scalarX.transform(X)
y = scalarY.transform(y.reshape(100,1))
# define and fit the final model
model = Sequential()
model.add(Dense(4, input_dim=2, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='linear'))
model.compile(loss='mse', optimizer='adam')
model.fit(X, y, epochs=1000, verbose=0)
############# 
# new instances where we do not know the answer
Xnew, a = make_regression(n_samples=3, n_features=2, noise=0.1, random_state=1)
Xnew = scalarX.transform(Xnew)
# make a prediction
ynew = model.predict(Xnew)
# show the inputs and predicted outputs
for i in range(len(Xnew)):
	print("X=%s, Predicted=%s" % (Xnew[i], ynew[i]))
############ single prediction
# new instance where we do not know the answer
Xnew = array([[0.29466096, 0.30317302]])
# make a prediction
ynew = model.predict(Xnew)
# show the inputs and predicted outputs
print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))

-------------------------------
